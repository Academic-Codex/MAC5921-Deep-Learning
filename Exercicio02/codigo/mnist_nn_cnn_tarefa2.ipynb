{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "485ebd8a",
   "metadata": {},
   "source": [
    "\n",
    "# MAC05921 — Tarefa 2: MNIST (NN × CNN) com PyTorch\n",
    "\n",
    "Este notebook executa a exploração pedida: treinar uma **Rede Neural (NN)** e uma **Rede Neural Convolucional (CNN)** no MNIST, \n",
    "comparando comportamento de *loss*, overfitting, *early stopping*, e desempenho em validação/teste.\n",
    "Ele também **gera figuras** e um **CSV de resultados** para facilitar o relatório em LaTeX.\n",
    "\n",
    "> **Como usar**\n",
    "> 1. Garanta internet (para baixar o MNIST) e GPU se possível.\n",
    "> 2. Rode as células em sequência.\n",
    "> 3. Ao final, as figuras ficam em `figures/` e os resultados em `resultados.csv`.\n",
    "> 4. Opcional: rode as explorações extras (ruído, desbalanceamento, t‑SNE, Grad‑CAM).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d6afc3",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15993cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 1.10.1+cu102\n",
      "CUDA available? True\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Versões e imports\n",
    "import sys, os, math, time, random, csv\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available?\", torch.cuda.is_available())\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "BASE = Path(\".\")\n",
    "OUT_FIG = BASE / \"figures\"\n",
    "OUT_FIG.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac9f16d",
   "metadata": {},
   "source": [
    "\n",
    "## Configuração básica\n",
    "\n",
    "- `samples_per_class`: número de exemplos por classe usados para **treino** (controle de orçamento).\n",
    "- `val_split`: fração do *train* dedicada à **validação**.\n",
    "- `epochs`: máximo de épocas; *early stopping* pode parar antes.\n",
    "- `patience`: paciência do *early stopping* (nº de épocas sem melhora na validação).\n",
    "- `batch_size`, `lr`: como de costume.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13adb29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hiperparâmetros iniciais\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "samples_per_class = 300      # ajuste nas repetições (por ex. 50, 100, 300, 1000...)\n",
    "val_split = 0.2\n",
    "epochs = 30\n",
    "patience = 5\n",
    "batch_size = 128\n",
    "lr = 1e-3\n",
    "\n",
    "# Normalização padrão do MNIST\n",
    "mean, std = (0.1307,), (0.3081,)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19d81c5",
   "metadata": {},
   "source": [
    "\n",
    "## Dados: MNIST + subamostragem por classe\n",
    "\n",
    "Baixamos o MNIST via `torchvision.datasets.MNIST`. Para o treino, usamos **apenas** `samples_per_class` por classe.\n",
    "Se quiser explorar **ruído** (sal‑pimenta) e **deslocamentos/escala**, ative o bloco de **Data Augmentation** mais adiante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd9cce75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35.4%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "74.3%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Treino: 2400 | Validação: 600 | Teste: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Download (requer internet local/Colab/Kaggle)\n",
    "DATA_DIR = BASE / \"data\"\n",
    "DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "full_train = datasets.MNIST(root=DATA_DIR, train=True, download=True, transform=train_transform)\n",
    "full_test  = datasets.MNIST(root=DATA_DIR, train=False, download=True, transform=test_transform)\n",
    "\n",
    "# Subamostragem balanceada para o treino (samples_per_class por classe)\n",
    "labels = np.array(full_train.targets)\n",
    "indices_per_class = [np.where(labels == c)[0] for c in range(10)]\n",
    "\n",
    "chosen_indices = []\n",
    "for c in range(10):\n",
    "    idxs = indices_per_class[c]\n",
    "    rng.shuffle(idxs)\n",
    "    chosen_indices.extend(idxs[:samples_per_class])\n",
    "chosen_indices = np.array(chosen_indices)\n",
    "rng.shuffle(chosen_indices)\n",
    "\n",
    "subset_train = Subset(full_train, chosen_indices.tolist())\n",
    "\n",
    "# Split treino/validação\n",
    "n_total = len(subset_train)\n",
    "n_val = int(val_split * n_total)\n",
    "n_train = n_total - n_val\n",
    "train_set, val_set = random_split(subset_train, [n_train, n_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_set,   batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(full_test, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Treino: {len(train_set)} | Validação: {len(val_set)} | Teste: {len(full_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fefc8f",
   "metadata": {},
   "source": [
    "\n",
    "## Modelos\n",
    "\n",
    "Definimos:\n",
    "- **MLP (NN)** simples: `Flatten -> Linear -> ReLU -> ... -> Linear` \n",
    "- **CNN** compacta: conv → ReLU → pool → conv → ReLU → pool → Linear(s)\n",
    "\n",
    "Também computamos o **número de parâmetros** para manter NN e CNN em ordem de grandeza semelhante.\n",
    "Ajuste `hidden_mlp`, `channels_cnn`, etc., para aproximar os totais.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b60d019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parâmetros MLP: 269322\n",
      "Parâmetros CNN: 206922\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden=256, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, c1=16, c2=32, fc=128, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, c1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(c1, c2, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear((28//4)*(28//4)*c2, fc)\n",
    "        self.fc2 = nn.Linear(fc, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))   # 28x28 -> 28x28\n",
    "        x = self.pool(x)            # -> 14x14\n",
    "        x = F.relu(self.conv2(x))   # 14x14 -> 14x14\n",
    "        x = self.pool(x)            # -> 7x7\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "hidden_mlp = 256\n",
    "cnn_c1, cnn_c2, cnn_fc = 16, 32, 128\n",
    "\n",
    "mlp = MLP(hidden=hidden_mlp).to(DEVICE)\n",
    "cnn = SmallCNN(c1=cnn_c1, c2=cnn_c2, fc=cnn_fc).to(DEVICE)\n",
    "\n",
    "print(\"Parâmetros MLP:\", count_params(mlp))\n",
    "print(\"Parâmetros CNN:\", count_params(cnn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95a573d",
   "metadata": {},
   "source": [
    "\n",
    "## Laço de treino, avaliação e *early stopping*\n",
    "\n",
    "Treinamos com `CrossEntropyLoss`. O *early stopping* monitora a **loss de validação**, salvando o melhor modelo.\n",
    "Também geramos **curvas de loss/acc**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73df2c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best = float(\"inf\")\n",
    "        self.count = 0\n",
    "        self.stop = False\n",
    "\n",
    "    def step(self, value):\n",
    "        if value < self.best - self.min_delta:\n",
    "            self.best = value\n",
    "            self.count = 0\n",
    "        else:\n",
    "            self.count += 1\n",
    "            if self.count >= self.patience:\n",
    "                self.stop = True\n",
    "\n",
    "def train_one_epoch(model, loader, optim, criterion):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        optim.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_loss += float(loss.item()) * y.size(0)\n",
    "        pred = logits.argmax(1)\n",
    "        correct += int((pred == y).sum().item())\n",
    "        total += y.size(0)\n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    all_y, all_p = [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        total_loss += float(loss.item()) * y.size(0)\n",
    "        pred = logits.argmax(1)\n",
    "        correct += int((pred == y).sum().item())\n",
    "        total += y.size(0)\n",
    "        all_y.append(y.cpu().numpy())\n",
    "        all_p.append(pred.cpu().numpy())\n",
    "    return total_loss/total, correct/total, np.concatenate(all_y), np.concatenate(all_p)\n",
    "\n",
    "def run_training(model, name=\"model\", epochs=30, lr=1e-3, patience=5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    es = EarlyStopping(patience=patience)\n",
    "    history = {\"epoch\": [], \"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    best_state = None\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optim, criterion)\n",
    "        va_loss, va_acc, _, _ = evaluate(model, val_loader, criterion)\n",
    "        history[\"epoch\"].append(ep)\n",
    "        history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
    "        history[\"val_loss\"].append(va_loss);   history[\"val_acc\"].append(va_acc)\n",
    "        print(f\"[{name}] epoch {ep:02d} | train_loss={tr_loss:.4f} acc={tr_acc:.4f} | val_loss={va_loss:.4f} acc={va_acc:.4f}\")\n",
    "\n",
    "        if va_loss <= es.best - es.min_delta:\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        es.step(va_loss)\n",
    "        if es.stop:\n",
    "            print(f\"Early stopping em {ep}. Melhor val_loss={es.best:.4f}.\")\n",
    "            break\n",
    "\n",
    "    # restaura melhor\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return history\n",
    "\n",
    "def plot_curves(hist, title, savepath):\n",
    "    # Um gráfico por figura, sem estilos/cores custom (diretriz)\n",
    "    fig = plt.figure()\n",
    "    xs = hist[\"epoch\"]\n",
    "    plt.plot(xs, hist[\"train_loss\"], label=\"train_loss\")\n",
    "    plt.plot(xs, hist[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.xlabel(\"Época\"); plt.ylabel(\"Loss\"); plt.title(title + \" — Loss\")\n",
    "    plt.legend()\n",
    "    fig.savefig(savepath.replace(\".png\", \"_loss.png\"), bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(xs, hist[\"train_acc\"], label=\"train_acc\")\n",
    "    plt.plot(xs, hist[\"val_acc\"], label=\"val_acc\")\n",
    "    plt.xlabel(\"Época\"); plt.ylabel(\"Acurácia\"); plt.title(title + \" — Acurácia\")\n",
    "    plt.legend()\n",
    "    fig.savefig(savepath.replace(\".png\", \"_acc.png\"), bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c5fffe",
   "metadata": {},
   "source": [
    "\n",
    "## Experimento principal\n",
    "\n",
    "- Treinar **MLP** e **CNN** no *subset* configurado.\n",
    "- *Early stopping* por `val_loss`.\n",
    "- Curvas de **loss/acc** salvas em `figures/`.\n",
    "- Avaliar no **teste** e gerar **matriz de confusão** + **relatório**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eb2439e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLP] epoch 01 | train_loss=1.2727 acc=0.6596 | val_loss=0.6184 acc=0.7867\n",
      "[MLP] epoch 02 | train_loss=0.4558 acc=0.8554 | val_loss=0.3915 acc=0.8817\n",
      "[MLP] epoch 03 | train_loss=0.3105 acc=0.9083 | val_loss=0.3064 acc=0.9100\n",
      "[MLP] epoch 04 | train_loss=0.2284 acc=0.9358 | val_loss=0.2923 acc=0.9133\n",
      "[MLP] epoch 05 | train_loss=0.1878 acc=0.9425 | val_loss=0.3004 acc=0.9067\n",
      "[MLP] epoch 06 | train_loss=0.1455 acc=0.9600 | val_loss=0.2478 acc=0.9267\n",
      "[MLP] epoch 07 | train_loss=0.1111 acc=0.9738 | val_loss=0.2528 acc=0.9250\n",
      "[MLP] epoch 08 | train_loss=0.0925 acc=0.9754 | val_loss=0.2448 acc=0.9250\n",
      "[MLP] epoch 09 | train_loss=0.0679 acc=0.9821 | val_loss=0.2277 acc=0.9283\n",
      "[MLP] epoch 10 | train_loss=0.0540 acc=0.9879 | val_loss=0.2262 acc=0.9317\n",
      "[MLP] epoch 11 | train_loss=0.0401 acc=0.9933 | val_loss=0.2723 acc=0.9183\n",
      "[MLP] epoch 12 | train_loss=0.0320 acc=0.9938 | val_loss=0.2695 acc=0.9250\n",
      "[MLP] epoch 13 | train_loss=0.0207 acc=0.9975 | val_loss=0.2461 acc=0.9400\n",
      "[MLP] epoch 14 | train_loss=0.0132 acc=0.9992 | val_loss=0.2302 acc=0.9400\n",
      "[MLP] epoch 15 | train_loss=0.0103 acc=0.9996 | val_loss=0.2359 acc=0.9400\n",
      "Early stopping em 15. Melhor val_loss=0.2262.\n",
      "\n",
      "Relatório de classificação — MLP:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9560    0.9531    0.9545       980\n",
      "           1     0.9752    0.9718    0.9735      1135\n",
      "           2     0.9148    0.9157    0.9153      1032\n",
      "           3     0.9331    0.9119    0.9224      1010\n",
      "           4     0.9368    0.8910    0.9134       982\n",
      "           5     0.9185    0.8969    0.9075       892\n",
      "           6     0.9022    0.9624    0.9313       958\n",
      "           7     0.9445    0.9105    0.9272      1028\n",
      "           8     0.8795    0.8994    0.8893       974\n",
      "           9     0.8781    0.9207    0.8989      1009\n",
      "\n",
      "    accuracy                         0.9241     10000\n",
      "   macro avg     0.9239    0.9233    0.9233     10000\n",
      "weighted avg     0.9247    0.9241    0.9242     10000\n",
      "\n",
      "[CNN] epoch 01 | train_loss=1.6287 acc=0.5633 | val_loss=0.7524 acc=0.8167\n",
      "[CNN] epoch 02 | train_loss=0.5555 acc=0.8317 | val_loss=0.4275 acc=0.8717\n",
      "[CNN] epoch 03 | train_loss=0.3704 acc=0.8879 | val_loss=0.3266 acc=0.8983\n",
      "[CNN] epoch 04 | train_loss=0.2813 acc=0.9129 | val_loss=0.2911 acc=0.9083\n",
      "[CNN] epoch 05 | train_loss=0.2379 acc=0.9242 | val_loss=0.2173 acc=0.9317\n",
      "[CNN] epoch 06 | train_loss=0.1794 acc=0.9458 | val_loss=0.1912 acc=0.9450\n",
      "[CNN] epoch 07 | train_loss=0.1404 acc=0.9583 | val_loss=0.1805 acc=0.9483\n",
      "[CNN] epoch 08 | train_loss=0.1117 acc=0.9692 | val_loss=0.1686 acc=0.9433\n",
      "[CNN] epoch 09 | train_loss=0.0908 acc=0.9754 | val_loss=0.1493 acc=0.9550\n",
      "[CNN] epoch 10 | train_loss=0.0848 acc=0.9771 | val_loss=0.1298 acc=0.9600\n",
      "[CNN] epoch 11 | train_loss=0.0642 acc=0.9846 | val_loss=0.1559 acc=0.9500\n",
      "[CNN] epoch 12 | train_loss=0.0643 acc=0.9788 | val_loss=0.1511 acc=0.9483\n",
      "[CNN] epoch 13 | train_loss=0.0468 acc=0.9862 | val_loss=0.1437 acc=0.9617\n",
      "[CNN] epoch 14 | train_loss=0.0382 acc=0.9929 | val_loss=0.1542 acc=0.9600\n",
      "[CNN] epoch 15 | train_loss=0.0301 acc=0.9950 | val_loss=0.1301 acc=0.9600\n",
      "Early stopping em 15. Melhor val_loss=0.1298.\n",
      "\n",
      "Relatório de classificação — CNN:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9719    0.9898    0.9808       980\n",
      "           1     0.9831    0.9753    0.9792      1135\n",
      "           2     0.9205    0.9535    0.9367      1032\n",
      "           3     0.9575    0.9584    0.9579      1010\n",
      "           4     0.9747    0.9399    0.9570       982\n",
      "           5     0.9624    0.9753    0.9688       892\n",
      "           6     0.9839    0.9582    0.9709       958\n",
      "           7     0.9711    0.9163    0.9429      1028\n",
      "           8     0.9108    0.9538    0.9318       974\n",
      "           9     0.9315    0.9435    0.9375      1009\n",
      "\n",
      "    accuracy                         0.9563     10000\n",
      "   macro avg     0.9567    0.9564    0.9564     10000\n",
      "weighted avg     0.9569    0.9563    0.9564     10000\n",
      "\n",
      "Figuras salvas em: figures\n",
      "Resultados em: resultados.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def confusion_and_report(y_true, y_pred, prefix=\"mlp\"):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(10)))\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(f\"Matriz de confusão — {prefix.upper()}\")\n",
    "    plt.xlabel(\"Predito\"); plt.ylabel(\"Verdadeiro\")\n",
    "    fig.savefig(OUT_FIG / f\"cm_{prefix}.png\", bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"\\nRelatório de classificação — {prefix.upper()}:\\n\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "# ----- MLP -----\n",
    "mlp = MLP(hidden=hidden_mlp).to(DEVICE)\n",
    "hist_mlp = run_training(mlp, name=\"MLP\", epochs=epochs, lr=lr, patience=patience)\n",
    "plot_curves(hist_mlp, \"MLP\", str(OUT_FIG / \"mlp_curves.png\"))\n",
    "_, _, y_true_val, y_pred_val = evaluate(mlp, val_loader, nn.CrossEntropyLoss())\n",
    "_, _, y_true_test, y_pred_test = evaluate(mlp, test_loader, nn.CrossEntropyLoss())\n",
    "confusion_and_report(y_true_test, y_pred_test, prefix=\"mlp\")\n",
    "\n",
    "# ----- CNN -----\n",
    "cnn = SmallCNN(c1=cnn_c1, c2=cnn_c2, fc=cnn_fc).to(DEVICE)\n",
    "hist_cnn = run_training(cnn, name=\"CNN\", epochs=epochs, lr=lr, patience=patience)\n",
    "plot_curves(hist_cnn, \"CNN\", str(OUT_FIG / \"cnn_curves.png\"))\n",
    "_, _, y_true_val_c, y_pred_val_c = evaluate(cnn, val_loader, nn.CrossEntropyLoss())\n",
    "_, _, y_true_test_c, y_pred_test_c = evaluate(cnn, test_loader, nn.CrossEntropyLoss())\n",
    "confusion_and_report(y_true_test_c, y_pred_test_c, prefix=\"cnn\")\n",
    "\n",
    "# Salva CSV resumido para o relatório\n",
    "import csv\n",
    "with open(BASE / \"resultados.csv\", \"w\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"modelo\",\"best_val_loss\",\"final_val_acc\",\"test_acc\",\"params\"])\n",
    "    w.writerow([\"MLP\", min(hist_mlp[\"val_loss\"]), hist_mlp[\"val_acc\"][-1], (y_true_test==y_pred_test).mean(), count_params(mlp)])\n",
    "    w.writerow([\"CNN\", min(hist_cnn[\"val_loss\"]), hist_cnn[\"val_acc\"][-1], (y_true_test_c==y_pred_test_c).mean(), count_params(cnn)])\n",
    "print(\"Figuras salvas em:\", OUT_FIG)\n",
    "print(\"Resultados em: resultados.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2abfa4",
   "metadata": {},
   "source": [
    "\n",
    "## Extras (opcionais)\n",
    "\n",
    "### 1) t‑SNE dos *embeddings*\n",
    "Projeta *features* (antes da última camada) para 2D com t‑SNE e plota.\n",
    "\n",
    "### 2) Ruído \"sal‑pimenta\" e *data augmentation*\n",
    "Ative a célula para reconstruir *loaders* com uma `Transform` que injeta ruído.\n",
    "\n",
    "### 3) Desbalanceamento de classes\n",
    "Mostra como reduzir amostras de algumas classes.\n",
    "\n",
    "### 4) Grad‑CAM (CNN)\n",
    "Implementação leve para ver regiões de ativação.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e776880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(model, loader, is_cnn):\n",
    "    model.eval()\n",
    "    feats, labels = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(DEVICE)\n",
    "        if is_cnn:\n",
    "            # Pega antes da última FC\n",
    "            x1 = F.relu(model.conv1(x)); x1 = model.pool(x1)\n",
    "            x2 = F.relu(model.conv2(x1)); x2 = model.pool(x2)\n",
    "            flat = torch.flatten(x2, 1)\n",
    "            f = F.relu(model.fc1(flat))\n",
    "        else:\n",
    "            flat = torch.flatten(x, 1)\n",
    "            f = F.relu(model.net[1](flat))  # após primeira FC do MLP\n",
    "        feats.append(f.cpu().numpy())\n",
    "        labels.append(y.numpy())\n",
    "    return np.vstack(feats), np.concatenate(labels)\n",
    "\n",
    "def tsne_plot(model, is_cnn, prefix=\"mlp\"):\n",
    "    X, y = extract_features(model, test_loader, is_cnn)\n",
    "    tsne = TSNE(n_components=2, init=\"random\", perplexity=30, learning_rate=\"auto\", n_iter=1000, verbose=1)\n",
    "    Z = tsne.fit_transform(X)\n",
    "    fig = plt.figure()\n",
    "    for c in range(10):\n",
    "        m = y==c\n",
    "        plt.scatter(Z[m,0], Z[m,1], s=5, label=str(c))\n",
    "    plt.title(f\"t-SNE features — {prefix.upper()}\")\n",
    "    plt.legend(markerscale=3, bbox_to_anchor=(1.05,1), loc=\"upper left\")\n",
    "    fig.savefig(OUT_FIG / f\"tsne_{prefix}.png\", bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "# Exemplo de uso (rodem se quiserem, pode levar alguns minutos):\n",
    "# tsne_plot(mlp, is_cnn=False, prefix=\"mlp\")\n",
    "# tsne_plot(cnn, is_cnn=True,  prefix=\"cnn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "214cde1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exemplo de ruído sal‑pimenta\n",
    "class SaltPepper(object):\n",
    "    def __init__(self, amount=0.05):\n",
    "        self.amount = amount\n",
    "    def __call__(self, img):\n",
    "        x = np.array(img)\n",
    "        m = rng.random(x.shape) < self.amount\n",
    "        sp = rng.integers(0, 2, size=x.shape) * 255  # 0 ou 255\n",
    "        x = x.copy()\n",
    "        x[m] = sp[m]\n",
    "        import PIL.Image as Image\n",
    "        return Image.fromarray(x.astype(np.uint8))\n",
    "\n",
    "# Para ativar, reconstrua os datasets/loaders (exemplo):\n",
    "# aug_transform = transforms.Compose([SaltPepper(0.05), transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "# full_train_aug = datasets.MNIST(root=DATA_DIR, train=True, download=True, transform=aug_transform)\n",
    "# (refaça a lógica de subset/split e loaders a partir de full_train_aug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd805a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Grad-CAM muito simples para a última conv da CNN\n",
    "class GradCAM:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.h1 = model.conv2.register_forward_hook(self._forward_hook)\n",
    "        self.h2 = model.conv2.register_full_backward_hook(self._backward_hook)\n",
    "    def _forward_hook(self, module, inp, out):\n",
    "        self.activations = out.detach()\n",
    "    def _backward_hook(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0].detach()\n",
    "    def __call__(self, x, class_idx=None):\n",
    "        self.model.zero_grad(set_to_none=True)\n",
    "        logits = self.model(x)\n",
    "        if class_idx is None:\n",
    "            class_idx = logits.argmax(1)\n",
    "        loss = logits[range(x.size(0)), class_idx].sum()\n",
    "        loss.backward()\n",
    "        grads = self.gradients # [B, C, H, W]\n",
    "        acts  = self.activations\n",
    "        weights = grads.mean(dim=(2,3), keepdim=True)\n",
    "        cam = (weights * acts).sum(dim=1, keepdim=True)\n",
    "        cam = F.relu(cam)\n",
    "        # normaliza para [0,1]\n",
    "        cam -= cam.min(dim=2, keepdim=True)[0].min(dim=3, keepdim=True)[0]\n",
    "        cam /= (cam.max(dim=2, keepdim=True)[0].max(dim=3, keepdim=True)[0] + 1e-8)\n",
    "        return cam\n",
    "\n",
    "# Exemplo de uso (depois de treinar a CNN):\n",
    "# gradcam = GradCAM(cnn)\n",
    "# x, y = next(iter(test_loader))\n",
    "# x = x.to(DEVICE)[:8]\n",
    "# cams = gradcam(x)  # [B,1,7,7] -> upscale para 28x28 se quiser visualizar por cima\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
